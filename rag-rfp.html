<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>RFP RAG â€” Stacked Retrievers | Aaron (ë°°ë™ìš°)</title>
  <style>
    :root {
      --bg-dark: #0f172a;
      --bg-alt1: #111827;
      --bg-alt2: linear-gradient(180deg, #0f172a 0%, #1e293b 100%);
      --card-dark: #1f2937;
      --text-light: #f3f4f6;
      --accent-blue: #3b82f6;
      --shadow-dark: 0 2px 10px rgba(0,0,0,0.4);
      --hover-glow: 0 0 12px rgba(59,130,246,0.3);
    }

    * { box-sizing: border-box; scroll-behavior: smooth; }
    body {
      font-family: "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      background-color: var(--bg-dark);
      color: var(--text-light);
      margin: 0;
      line-height: 1.6;
    }

    nav {
      position: fixed;
      top: 0;
      width: 100%;
      background-color: rgba(15, 23, 42, 0.9);
      backdrop-filter: blur(6px);
      display: flex;
      justify-content: center;
      gap: 2rem;
      padding: 1rem 0.5rem;
      z-index: 1000;
      border-bottom: 1px solid rgba(59,130,246,0.2);
    }

    nav a { color: var(--text-light); text-decoration: none; font-weight: 500; transition: color 0.3s; }
    nav a:hover { color: var(--accent-blue); }

    header {
      background: linear-gradient(90deg, #2563eb, #0891b2);
      color: white;
      text-align: center;
      padding: 7rem 1rem 2rem;
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .profile-photo {
      width: 140px;
      height: 140px;
      border-radius: 50%;
      object-fit: cover;
      border: 4px solid white;
      box-shadow: 0 4px 10px rgba(0,0,0,0.3);
      margin-bottom: 1rem;
    }

    h1 { font-size: 2rem; margin-bottom: 0.25rem; text-align: center; }
    h2 { margin-top: 1.5rem; border-bottom: 2px solid #374151; padding-bottom: 0.3rem; color: white; }

    main { max-width: 1000px; margin: 2rem auto; padding: 0 1.5rem; }
    section { padding: 2rem 1rem; border-radius: 2rem; margin: 2.5rem 0; transition: background 0.3s; }

    section:nth-of-type(odd) { background: var(--bg-alt1); box-shadow: inset 0 0 40px rgba(0,0,0,0.3); }
    section:nth-of-type(even) { background: var(--bg-alt2); box-shadow: inset 0 0 40px rgba(0,0,0,0.2); }

    a { color: #60a5fa; text-decoration: none; }
    a:hover { text-decoration: underline; }

    .dual { margin-top: 0.5rem; color: #a1a1aa; font-size: 0.95em; display: block; }

    /* Lightbox styles */
    #lightbox {
      position: fixed;
      display: none;
      top: 0; left: 0; width: 100%; height: 100%;
      background: rgba(0,0,0,0.9);
      justify-content: center;
      align-items: center;
      z-index: 2000;
    }

    #lightbox img {
      max-width: 90%;
      max-height: 90%;
      border-radius: 1rem;
      box-shadow: 0 0 30px rgba(255,255,255,0.4);
      transition: transform 0.3s ease;
    }

    #lightbox:target { display: flex; }

    .metric-table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 0.9rem;
      margin-bottom: 1rem;
    }
    .metric-table th, .metric-table td {
      border: 1px solid #334155;
      padding: 0.6rem;
      text-align: center;
    }
    .metric-table th { background: #1e293b; color: #93c5fd; }

    pre.code {
      background: #0b1220;
      border: 1px solid #334155;
      color: #dbeafe;
      padding: 1rem;
      border-radius: 0.5rem;
      overflow: auto;
      font-family: Consolas, "Courier New", monospace;
      font-size: 0.92em;
    }

    figure { text-align: center; margin: 1rem 0; }
    figure img {
      width: 100%;
      max-height: 460px;
      object-fit: contain;
      border-radius: 0.5rem;
      box-shadow: 0 12px 30px rgba(0,0,0,0.45);
    }
    figcaption { margin-top: 0.6rem; color: #9ca3af; font-size: 0.95rem; }

    .footer {
      text-align: center;
      padding: 2rem 1rem;
      font-size: 0.9rem;
      color: #9ca3af;
      background-color: #0d162b;
      border-top-left-radius: 2rem;
      border-top-right-radius: 2rem;
    }

    code {
      background-color: #0b1220;
      color: #fb923c;
      padding: 2px 6px;
      border-radius: 6px;
      font-family: Consolas, "Courier New", monospace;
      font-size: 0.9em;
    }

    ul { margin: 0.8rem 0 0.8rem 1.2rem; }
    li { margin-bottom: 0.45rem; }
    .note { color: #9ca3af; font-size: 0.9rem; font-style: italic; }

    @media (max-width:800px) {
      main { max-width: 92%; }
      header { padding: 5.8rem 1rem 2rem; }
      .profile-photo { width: 120px; height: 120px; }
    }
  </style>
</head>
<body>
  <nav>
    <a href="index.html">Home</a>
    <a href="#overview">Overview</a>
    <a href="#role">Role</a>
    <a href="#parser">Parser Comparison</a>
    <a href="#embeddings">Embeddings</a>
    <a href="#evaluation">Evaluation</a>
    <a href="#faiss">FAISS</a>
    <a href="#next">Reflection</a>
    <a href="#contact">Contact</a>
  </nav>

  <header>
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/800px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg" alt="Profile photo" class="profile-photo" />
    <h1>Request for Proposal â€” Stacked Retrievers (RAG)</h1>
    <p>100ë¶€ í•œêµ­ì–´ ì œì•ˆìš”ì²­ì„œ ë¬¸ì„œ ê¸°ë°˜ì˜ Retrieval-Augmented Generation ì‹¤í—˜</p>
    <div class="badges">
      <img src="https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white" alt="Python Badge" />
      <img src="https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white" alt="PyTorch Badge" />
      <img src="https://img.shields.io/badge/LangChain-121212?logo=langchain&logoColor=white" alt="LangChain" />
      <img src="https://img.shields.io/badge/OpenAI-412991?logo=openai&logoColor=white" alt="OpenAI Badge" />
      <img src="https://img.shields.io/badge/Chroma-FF5733?logo=chromadb&logoColor=white" alt="Chroma Badge" />
      <img src="https://img.shields.io/badge/FAISS-0052CC?logo=faiss&logoColor=white" alt="FAISS Badge" />
      <img src="https://img.shields.io/badge/HuggingFace-FFD21E?logo=huggingface&logoColor=black" alt="Hugging Face" />
    </div>
  </header>

  <main>
    <section id="overview">
      <h2>Overview</h2>
      <p>
        Built and evaluated a Retrieval-Augmented Generation (RAG) system for 100 Korean Request For Proposal (RFP) documents.
        Focus: parser comparison (Unstructured IO, MuPDF, pdfplumber), embedding model comparison (SNUNLP S-BERT vs OpenAI text-embedding-ada-002),
        and stacked retriever experiments with FAISS.
        <span class="dual">100ë¶€ì˜ í•œêµ­ì–´ ì œì•ˆìš”ì²­ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„Â·í‰ê°€í–ˆìŠµë‹ˆë‹¤. íŒŒì„œ, ì„ë² ë”©, ìŠ¤íƒí˜• ê²€ìƒ‰ê¸° ë¹„êµê°€ ì£¼ìš” í¬ì¸íŠ¸ì…ë‹ˆë‹¤.</span>
      </p>
    </section>

    <section id="role">
      <h2>Role and Pre-Processing</h2>
      <p>
        Developed preprocessing tooling and led parser/embedding experiments: converted 98 HWP â†’ PDF (Python), handled conversion anomalies (42 files required manual fixes), and normalized chunking for embedding.
        <span class="dual">ì „ì²˜ë¦¬ ë„êµ¬ë¥¼ ê°œë°œí•˜ê³  íŒŒì„œ/ì„ë² ë”© ì‹¤í—˜ì„ ì£¼ë„í–ˆìŠµë‹ˆë‹¤. 98ë¶€ì˜ HWP íŒŒì¼ì„ ìë™ìœ¼ë¡œ PDFë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í–ˆê³ ,
        ë³€í™˜ ì¤‘ 42ë¶€ ë¬¸ì„œì—ì„œ 'í•œ í˜ì´ì§€ì— ë‘ í˜ì´ì§€ê°€ ë³‘í•©'ë˜ëŠ” ì˜¤ë¥˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë³´ì •í•´ ì •ìƒ íŒŒì‹±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.</span>
      </p>
    </section>

    <section id="parser">
      <h2>Parser Comparison (Unstructured IO / MuPDF / pdfplumber)</h2>
      <p>
        Compared three parsing approaches. Unstructured IO sometimes flattened table semantics, so MuPDF + pdfplumber was adopted for more reliable structure extraction.
        <span class="dual">ì„¸ ê°€ì§€ íŒŒì„œë¥¼ ë¹„êµí•œ ê²°ê³¼, Unstructured IOëŠ” í‘œ/ë„í‘œì˜ í–‰Â·ì—´ êµ¬ì¡° ì •ë³´ê°€ ì œëŒ€ë¡œ ë³´ì¡´ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì–´ ìµœì¢…ì ìœ¼ë¡œ MuPDFì™€ pdfplumber ì¡°í•©ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.</span>
      </p>
      <p class="note">(Practical note: Unstructured IO can be strong for some layouts but may lose table row/column structure â€” see project report.)</p>
      <span class="dual">(ì‹¤ë¬´ ë…¸íŠ¸: Unstructured IOëŠ” íŠ¹ì • ë ˆì´ì•„ì›ƒ ì²˜ë¦¬ì— ê°•ì ì´ ìˆìœ¼ë‚˜ í‘œì˜ ì˜ë¯¸ì  êµ¬ì¡°ê°€ í‰íƒ„í™”ë˜ëŠ” ì‚¬ë¡€ê°€ ìˆì–´ ë³´ê³ ì„œì— ì¼€ì´ìŠ¤ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.)</span>
    </section>

    <section id="embeddings">
      <h2>Embedding experiments (S-BERT vs. OpenAI)</h2>
      <p>
        Two embedding pipelines compared:
        <ul>
          <li><strong>SNUNLP S-BERT</strong> (snunlp/KR-SBERT-V40K-klueNLI-augSTS) â†’ ChromaDB</li>
          <li><strong>OpenAI</strong> text-embedding-ada-002 â†’ ChromaDB</li>
        </ul>
        For each pipeline we plotted normalized vector length histograms and t-SNE on original embeddings to inspect distributional properties.
        <span class="dual">(1) ì„œìš¸ëŒ€í•™êµ S-BERT, (2) OpenAI text-embedding-ada-002ë¥¼ ê°ê° Chromaì— ì €ì¥í•˜ê³ , ì •ê·œí™”í•œ ë²¡í„° ê¸¸ì´ íˆìŠ¤í† ê·¸ë¨ê³¼ ì •ê·œí™”í•˜ì§€ ì•Šì€ ì„ë² ë”©ì˜ t-SNEë¥¼ ë¹„êµÂ·ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤.</span>
      </p>

      <figure>
        <img src="rag-rfp-parser.png" alt="Embedding vector length histogram placeholder" class="screenshot zoomable">
        <figcaption>
          Normalized vector length histogram and t-SNE plot for each parser: Unstructured (left), MuPDF (middle), pdfplumber (right).
          <span class="dual">
            ì‹¤ì œ ì´ë¯¸ì§€ëŠ” ê° íŒŒì„œ(Unstructured, MuPDF, pdfplumber) ë° ì„ë² ë”© ëª¨ë¸ë³„ë¡œ ìƒì„±ëœ ì •ê·œí™”ëœ ë²¡í„° ê¸¸ì´ íˆìŠ¤í† ê·¸ë¨Â·t-SNE í”Œë¡¯ì„ í¬í•¨í•©ë‹ˆë‹¤.
          </span>
        </figcaption>
      </figure>

      <figure>
        <img src="rag-rfp-sbert.png" alt="t-SNE embedding placeholder" class="screenshot zoomable">
        <figcaption>
          Histogram of normalized vector length (left), non-normalized vector length (middle), and t-SNE of non-normalized embeddings (right) â€” used to inspect semantic cluster structure.
          <span class="dual">
            ì •ê·œí™”ëœ ë²¡í„° ê¸¸ì´ íˆìŠ¤í† ê·¸ë¨(ì¢Œ), ë¹„ì •ê·œí™”ëœ ë²¡í„° ê¸¸ì´ íˆìŠ¤í† ê·¸ë¨(ì¤‘), ë¹„ì •ê·œí™”ëœ ì„ë² ë”©ì˜ t-SNE ê·¸ë˜í”„(ìš°).
          </span>
        </figcaption>
      </figure>
    </section>

    <section id="evaluation">
      <h2>22-Question Evaluation (Qualitative)</h2>
      <p>
        We prepared 22 domain-specific queries (example list below) and evaluated responses across three parser data-sets (Original / Plumber / MUP) using OpenAI's text-embedding-ada-002 retrieval pipeline. Responses were manually scored: <strong>+2 (richer), +1 (correct), -1 (incorrect or partially wrong)</strong>.
        <span class="dual">
          22ê°œì˜ ë„ë©”ì¸ ì§ˆë¬¸(ì•„ë˜ ì˜ˆì‹œ)ì„ ë§Œë“¤ì–´ 3ê°œ íŒŒì„œ ë°ì´í„°ì…‹(ê¸°ì¡´ / Plumber / MUP)ì—ì„œì˜ ì‘ë‹µì„ OpenAI ì„ë² ë”© ê¸°ë°˜ RAGë¡œ ì–»ì–´ ìˆ˜ë™ í‰ê°€í–ˆìŠµë‹ˆë‹¤. í‰ê°€ëŠ” <strong>+2(ì •ë³´ê°€ ë” í’ë¶€í•¨), +1(ì •í™•í•¨), -1(ì˜¤ë¥˜ í¬í•¨)</strong> ê¸°ì¤€ì…ë‹ˆë‹¤.
        </span>
      </p>

      <p><strong>Sample questions (selected):</strong></p>
      <ul>
        <li>êµ­ë¯¼ì—°ê¸ˆê³µë‹¨ì´ ë°œì£¼í•œ ì´ëŸ¬ë‹ì‹œìŠ¤í…œ ê´€ë ¨ ì‚¬ì—… ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•´ ì¤˜. / Summarize the e-learning project requirements by êµ­ë¯¼ì—°ê¸ˆê³µë‹¨.</li>
        <li>ê³µê³  ë²ˆí˜¸ê°€ 20240821893ì¸ ì‚¬ì—… ì´ë¦„ì´ ë­ì•¼? / What is the project title for announcement 20240821893?</li>
        <li>ì…ì°° ë§ˆê°ì¼ì´ 2024ë…„ 12ì›”ì¸ ì‚¬ì—…ì„ ì•Œë ¤ì¤˜. / List projects with bid close date in Dec 2024.</li>
        <li>í•œêµ­í•œì˜í•™ì—°êµ¬ì› í†µí•©ì •ë³´ ì‹œìŠ¤í…œ ê³ ë„í™” ìš©ì—­ ì‚¬ì—…ì˜ ëª¨ë“  ìš”êµ¬ì‚¬í•­ ëª©ë¡ì„ ì•Œë ¤ì¤˜. / List all requirement identifications and names for the Korean Institute of Oriental Medicine project.</li>
      </ul>

      <p class="note">
        Key qualitative observation: OpenAI + MUP data-set produced the most useful and specific answers overall; OpenAI+Plumber often omitted official announcement numbers in summaries and produced overly short summaries. Full scoring table is available in the project report.
        <span class="dual">
          ì •ì„±ì  ê´€ì°°: OpenAI+MUP ì¡°í•©ì´ ê°€ì¥ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ì‘ë‹µì„ ì œê³µí–ˆìœ¼ë©°, OpenAI+PlumberëŠ” ê³µê³ ë²ˆí˜¸ ëˆ„ë½ì´ë‚˜ ì§€ë‚˜ì¹˜ê²Œ ê°„ë‹¨í•œ ìš”ì•½ì„ ë³´ì´ëŠ” ê²½í–¥ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì „ì²´ í‰ì í‘œëŠ” ë³´ê³ ì„œì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        </span>
      </p>

      <table class="metric-table" aria-label="qualitative scoring example">
        <thead>
          <tr><th>Data-Set</th><th>+2 (richer)</th><th>+1 (correct)</th><th>-1 (incorrect)</th></tr>
        </thead>
        <tbody>
          <tr><td>OpenAI + MUP (example)</td><td>â€”</td><td>â€”</td><td>â€”</td></tr>
          <tr><td>OpenAI + Plumber (example)</td><td>â€”</td><td>â€”</td><td>â€”</td></tr>
        </tbody>
      </table>

      <p class="note" style="text-align:center;">*Table above is a placeholder showing structure â€” see the project report for the complete scoring matrix.* <span class="dual">ìœ„ í‘œëŠ” í˜•ì‹ ì˜ˆì‹œì´ë©° ì „ì²´ í‰ì  í‘œëŠ” í”„ë¡œì íŠ¸ ë³´ê³ ì„œì— ìˆ˜ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</span></p>
    </section>

    <section id="faiss">
      <h2>FAISS â€” Stacked Retriever Pipeline</h2>
      <p>
        Implemented a prototype stacked retriever (hybrid search â†’ multi-query â†’ re-rank â†’ contextual compression) and tested with Unstructured IO parsed docs and OpenAI embeddings. Components: MultiVectorRetriever, MultiQueryRetriever, BM25 hybrid, CrossEncoder re-ranker, LLM-based contextual compressor.
        <span class="dual">í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â†’ ë‹¤ì¤‘ ì§ˆì˜ â†’ ì¬ì •ë ¬ â†’ ë¬¸ë§¥ ì••ì¶• íë¦„ì˜ í”„ë¡œí† íƒ€ì…ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.</span>
      </p>

      <pre class="code"><code>def create_stacked_retriever(base_retriever):
    """Full pipeline: hybrid â†’ multi-query â†’ re-rank â†’ contextual compression.""" 
    return add_compression(
        add_reranking(
            add_multi_query(
                add_hybrid_search(base_retriever)
            )
        )
    )</code></pre>
    </section>

    <section id="next">
      <h2>Reflection & Next steps</h2>
      <p>
        Lessons: parser choice affects downstream retrieval; embedding normalization shapes retriever design; and human-scored qualitative evaluation is critical. Next steps: expand automatic table-structure parsing and fine-tune reranker models.
        <span class="dual">íŒŒì„œ ì„ íƒ, ì„ë² ë”© ì •ê·œí™”, ì •ì„±í‰ê°€ì˜ ì¤‘ìš”ì„±ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ëŠ” í‘œ êµ¬ì¡° ìë™í™” ë° ì¬ì •ë ¬ê¸° íŠœë‹ì…ë‹ˆë‹¤.</span>
      </p>

      <p class="note">Full notebooks, t-SNE/histograms, and evaluation matrices available in the repository and project report.</p>
      <span class="dual">ì „ì²´ ë…¸íŠ¸ë¶, t-SNE/íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™”, ê·¸ë¦¬ê³  í‰ê°€ ë§¤íŠ¸ë¦­ìŠ¤ëŠ” ì €ì¥ì†Œì™€ í”„ë¡œì íŠ¸ ë³´ê³ ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</span>
    </section>

    <section id="contact">
      <h2>Contact | ì—°ë½ì²˜</h2>
      <p>
        ğŸ’¼ <a href="https://github.com/hiiamaaron" target="_blank">GitHub Profile</a><br />
        ğŸ”— <a href="https://www.linkedin.com/in/raffaello-sanzio-da-urbino-703897345/" target="_blank">LinkedIn</a><br />
        ğŸ“§ <a href="mailto:projectares777@gmail.com">projectares777@gmail.com</a><br />
      </p>
    </section>
  </main>

  <div class="footer">
    Â© 2025 Aaron â€” Built with â¤ï¸ using GitHub Pages
  </div>

  <!-- ===== LIGHTBOX JS ===== -->
  <div id="lightbox" onclick="this.style.display='none'">
    <img id="lightbox-img" src="">
  </div>

  <script>
    const zoomableImages = document.querySelectorAll('.zoomable');
    const lightbox = document.getElementById('lightbox');
    const lightboxImg = document.getElementById('lightbox-img');

    zoomableImages.forEach(img => {
      img.addEventListener('click', () => {
        lightbox.style.display = 'flex';
        lightboxImg.src = img.src;
      });
    });
  </script>
</body>
</html>