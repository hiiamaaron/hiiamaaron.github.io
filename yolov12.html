<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>YOLOv12 Pill Detection | Aaron (ë°°ë™ìš°)</title>
  <style>
    :root {
      --bg-dark: #0f172a;
      --bg-alt1: #111827;
      --bg-alt2: linear-gradient(180deg, #0f172a 0%, #1e293b 100%);
      --card-dark: #1f2937;
      --text-light: #f3f4f6;
      --accent-blue: #3b82f6;
      --shadow-dark: 0 2px 10px rgba(0,0,0,0.4);
      --hover-glow: 0 0 12px rgba(59,130,246,0.3);
    }

    * { box-sizing: border-box; scroll-behavior: smooth; }
    body {
      font-family: "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      background-color: var(--bg-dark);
      color: var(--text-light);
      margin: 0;
      line-height: 1.6;
    }

    nav {
      position: fixed;
      top: 0;
      width: 100%;
      background-color: rgba(15, 23, 42, 0.9);
      backdrop-filter: blur(6px);
      display: flex;
      justify-content: center;
      gap: 2rem;
      padding: 1rem 0.5rem;
      z-index: 1000;
      border-bottom: 1px solid rgba(59,130,246,0.2);
    }

    nav a { color: var(--text-light); text-decoration: none; font-weight: 500; transition: color 0.3s; }
    nav a:hover { color: var(--accent-blue); }

    header {
      background: linear-gradient(90deg, #2563eb, #0891b2);
      color: white;
      text-align: center;
      padding: 7rem 1rem 2rem;
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .profile-photo {
      width: 140px;
      height: 140px;
      border-radius: 50%;
      object-fit: cover;
      border: 4px solid white;
      box-shadow: 0 4px 10px rgba(0,0,0,0.3);
      margin-bottom: 1rem;
    }

    h1 { font-size: 2rem; margin-bottom: 0.25rem; }
    h2 {
      margin-top: 1.5rem;
      border-bottom: 2px solid #374151;
      padding-bottom: 0.3rem;
      color: white;
    }

    main { max-width: 1000px; margin: 2rem auto; padding: 0 1.5rem; }
    section { padding: 2rem 1rem; border-radius: 2rem; margin: 2.5rem 0; transition: background 0.3s; }

    section:nth-of-type(odd) { background: var(--bg-alt1); box-shadow: inset 0 0 40px rgba(0,0,0,0.3); }
    section:nth-of-type(even) { background: var(--bg-alt2); box-shadow: inset 0 0 40px rgba(0,0,0,0.2); }

    .project-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(420px, 1fr)); gap: 1.5rem; margin-top: 2rem; }

    .card {
      background: var(--card-dark);
      border-radius: 1.2rem;
      padding: 1.5rem;
      box-shadow: var(--shadow-dark);
      transition: transform 0.2s ease, box-shadow 0.3s ease;
      cursor: pointer;
    }

    .card:hover { transform: translateY(-4px); box-shadow: var(--hover-glow); }

    .card h4 {
      text-align: center;
      color: #93c5fd;
      font-size: 1.3rem;
      margin-top: 0;
      margin-bottom: 1rem;
      letter-spacing: 0.5px;
      text-shadow: 0 0 6px rgba(147,197,253,0.5);
    }

    code {
      background-color: #0b1220;
      color: #fb923c;
      padding: 2px 6px;
      border-radius: 6px;
      font-family: Consolas, "Courier New", monospace;
      font-size: 0.9em;
    }

    a { color: #60a5fa; text-decoration: none; }
    a:hover { text-decoration: underline; }

    .dual { margin-top: 0.5rem; color: #a1a1aa; font-size: 0.95em; }

    .footer { text-align: center; padding: 2rem 1rem; font-size: 0.9rem; color: #9ca3af; background-color: #0d162b; border-top-left-radius: 2rem; border-top-right-radius: 2rem; }

    .badges img { margin-right: 6px; vertical-align: middle; }

    pre { background: #0b1220; padding: 0.5rem; border-radius: 0.5rem; overflow-x: auto; }

    img.screenshot { max-width: 100%; border-radius: 0.5rem; margin-top: 0.5rem; cursor: pointer; box-shadow: var(--shadow-dark); transition: transform 0.3s ease; }

    img.screenshot:hover { transform: scale(1.02); }

    video { max-width: 100%; height: auto; border-radius: 1rem; box-shadow: var(--shadow-dark); margin-top: 0.5rem; }

    /* Lightbox styles */
    #lightbox {
      position: fixed;
      display: none;
      top: 0; left: 0; width: 100%; height: 100%;
      background: rgba(0,0,0,0.9);
      justify-content: center;
      align-items: center;
      z-index: 2000;
    }

    #lightbox img {
      max-width: 90%;
      max-height: 90%;
      border-radius: 1rem;
      box-shadow: 0 0 30px rgba(255,255,255,0.4);
      transition: transform 0.3s ease;
    }

    #lightbox:target { display: flex; }

    .metric-table, .hyperparam-table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 0.9rem;
      margin-bottom: 1rem;
    }
    .metric-table th, .metric-table td, .hyperparam-table th, .hyperparam-table td {
      border: 1px solid #334155;
      padding: 0.6rem;
      text-align: center;
    }
    .metric-table th, .hyperparam-table th { background: #1e293b; color: #93c5fd; }
    .hyperparam-table td { background: var(--card-dark); color: var(--text-light); text-align: left; }

    figure { text-align: center; margin: 1rem 0; }
    figure img {
      width: 100%;
      max-height: 480px;
      object-fit: contain;
      border-radius: 1.2rem;
      box-shadow: var(--shadow-dark);
    }
    figcaption { margin-top: 0.6rem; color: #9ca3af; font-size: 0.95rem; }

    ul { margin: 0.8rem 0 0.8rem 1.2rem; }
    li { margin-bottom: 0.45rem; }

    @media (max-width:800px) {
      main { padding: 0 1rem; }
    }
  </style>
</head>
<body>
  <!-- ===== NAVIGATION ===== -->
  <nav>
    <a href="index.html">Home</a>
    <a href="yolov12.html#overview">Overview</a>
    <a href="yolov12.html#role">Role</a>
    <a href="yolov12.html#approach">Approach</a>
    <a href="yolov12.html#hyperparams">Hyper-Parameters</a>
    <a href="yolov12.html#performance">Performance</a>
    <a href="yolov12.html#results">Results</a>
    <a href="yolov12.html#perclass">Metrics</a>
    <a href="yolov12.html#reflection">Reflection</a>
    <a href="yolov12.html#contact">Contact</a>
  </nav>

  <!-- ===== HEADER ===== -->
  <header>
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/800px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg" alt="Profile photo" class="profile-photo" />
    <h1>YOLOv12 Pill Detection</h1>
    <p style="margin-top:0.5rem;">ê²½êµ¬ì•½ì œ ê°ì²´ íƒì§€ ë° ë¶„ë¥˜ ëª¨ë¸ ê°œë°œ â€” YOLOv12n, Optuna, W&B</p>
    <div class="badges">
      <img src="https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white" alt="Python Badge" />
      <img src="https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white" alt="PyTorch Badge" />
      <img src="https://img.shields.io/badge/YOLOv12-FF6B6B?logo=yolo&logoColor=white" alt="YOLOv12 Badge" />
      <img src="https://img.shields.io/badge/Optuna-FF9500?logo=optuna&logoColor=white" alt="Optuna Badge" />
      <img src="https://img.shields.io/badge/W&B-F7931E?logo=wandb&logoColor=white" alt="Weights & Biases Badge" />
    </div>
  </header>

  <main>
    <!-- ===== OVERVIEW ===== -->
    <section id="overview">
      <h2>Overview</h2>
      <p>
        Medical pill object detection using <strong>YOLOv12n</strong> on a custom dataset sourced from
        <a href="https://aihub.or.kr/aihubdata/data/view.do?dataSetSn=576" target="_blank" rel="noopener">AIí—ˆë¸Œ</a>.
        Models were optimized with Optuna and experiments tracked with Weights & Biases.
        <br /><span class="dual">AIí—ˆë¸Œ ê¸°ë°˜ì˜ ê²½êµ¬ì•½ì œ ì´ë¯¸ì§€ ë°ì´í„°ë¡œ YOLOv12nì„ í•™ìŠµÂ·ìµœì í™”í•˜ê³  Optuna ë° W&Bë¡œ ì‹¤í—˜ì„ ê´€ë¦¬í–ˆìŠµë‹ˆë‹¤.</span>
      </p>
    </section>

    <!-- ===== ROLE ===== -->
    <section id="role">
      <h2>Role & Contributions</h2>
      <p>
        Led dataset preparation (COCO-style conversion & YAML), implemented Optuna hyperparameter search, and produced validation visualizations via W&B.
        <br /><span class="dual">COCO í˜•ì‹ ë³€í™˜, Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë° W&B ì‹œê°í™”ë¥¼ ì£¼ë„í–ˆìŠµë‹ˆë‹¤.</span>
      </p>
    </section>

    <!-- ===== APPROACH ===== -->
    <section id="approach">
      <h2>Approach</h2>
      <p>
        Per-image .txt label generation and a generated <code>custom_data.yaml</code> (names, nc, path, train, val). To reduce overfitting, TS_2 and TS_4 were merged and the model retrained on 82 classes.
        <br /><span class="dual">ì´ë¯¸ì§€ë³„ ë¼ë²¨(.txt)ê³¼ <code>custom_data.yaml</code>ì„ ìƒì„±í–ˆê³ , ë°ì´í„° ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´ ì¼ë¶€ ì„¸íŠ¸ë¥¼ ë³‘í•©í–ˆìŠµë‹ˆë‹¤.</span>
      </p>

      <p>
        <strong>Optuna</strong> searched learning rates (<code>lr0</code>, <code>lrf</code>), momentum, weight_decay, and augmentation strengths (mosaic, mixup, hsv, flip, scale) over ~50 trials.
        <br /><span class="dual">ì•½ 50íšŒ íƒìƒ‰ìœ¼ë¡œ í•™ìŠµë¥ , ëª¨ë©˜í…€, ê°€ì¤‘ì¹˜ ê°ì‡  ë° ì¦ê°• ê°•ë„ë¥¼ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.</span>
      </p>
    </section>

    <!-- ===== HYPERPARAMS ===== -->
    <section id="hyperparams">
      <h2>Optimized Hyper-Parameters</h2>
      <p>The final set of optimized hyper-parameters from Optuna tuning:</p>
      <table class="hyperparam-table" aria-label="optimized hyper-parameters">
          <thead>
              <tr><th>Parameter</th><th>Value</th></tr>
          </thead>
          <tbody>
              <tr><td>batch</td><td>10</td></tr>
              <tr><td>lrf</td><td>0.65947884519220539</td></tr>
              <tr><td>box</td><td>7.357768428724136</td></tr>
              <tr><td>mixup</td><td>0.888719794258092</td></tr>
              <tr><td>cls</td><td>1.4935155411670802</td></tr>
              <tr><td>momentum</td><td>0.920754733304813</td></tr>
              <tr><td>cos_lr</td><td>False</td></tr>
              <tr><td>mosaic</td><td>0.424942708569014176</td></tr>
              <tr><td>degrees</td><td>2.576681849094956</td></tr>
              <tr><td>rect</td><td>False</td></tr>
              <tr><td>dropout</td><td>0.2598944893787225</td></tr>
              <tr><td>scale</td><td>0.2253061215349854</td></tr>
              <tr><td>fliplr</td><td>0.6191146718070866</td></tr>
              <tr><td>shear</td><td>1.032108506614807</td></tr>
              <tr><td>flipud</td><td>0.2592334970826654</td></tr>
              <tr><td>translate</td><td>0.1516721572534314</td></tr>
              <tr><td>hsv_h</td><td>0.0149852516735826</td></tr>
              <tr><td>warmup_bias_lr</td><td>0.738238285154656</td></tr>
              <tr><td>hsv_s</td><td>0.6559938030454813</td></tr>
              <tr><td>warmup_epoch</td><td>1</td></tr>
              <tr><td>hsv_v</td><td>0.7424515055576003</td></tr>
              <tr><td>warmup_momentum</td><td>0.879237345909938</td></tr>
              <tr><td>lr0</td><td>0.014778806469092174</td></tr>
              <tr><td>weight_decay</td><td>0.0009731833150844206</td></tr>
          </tbody>
      </table>
    </section>

    <!-- ===== PERFORMANCE ===== -->
    <section id="performance">
      <h2>Performance</h2>
      <p>
        Inference benchmark: <strong>24.16 FPS</strong> on 100 test images measured with an <strong>NVIDIA Tesla T4 GPU</strong> (Google Colab).
        <br /><span class="dual">ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬: NVIDIA Tesla T4ì—ì„œ í‰ê·  24.16 FPS.</span>
      </p>
    </section>

    <!-- ===== RESULTS ===== -->
    <section id="results">
      <h2>Results</h2>
      <p>Final validation summary after retraining:</p>
      <table class="metric-table" aria-label="validation metrics">
        <thead>
          <tr><th>Precision</th><th>Recall</th><th>mAP@0.5</th></tr>
        </thead>
        <tbody>
          <tr><td>0.9915</td><td>0.9939</td><td>0.9894</td></tr>
        </tbody>
      </table>

      <p>
        On unseen combined test sets (TS_3â€“TS_8), the retrained model obtained <strong>mAP@0.5 = 0.4586</strong>, up from baseline 0.3648 â€” indicating improved generalization after dataset merging.
        <br /><span class="dual">ë³´ì§€ ì•Šì€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ mAP@0.5ê°€ 0.4586ìœ¼ë¡œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.</span>
      </p>

      <figure>
        <img src="yolov12.webp" alt="YOLOv12 Training and Validation Curves" class="screenshot zoomable">
        <figcaption>
          Training/validation curves tracked via Weights & Biases. <strong>cls_loss</strong> = classification loss, <strong>dfl_loss</strong> = distribution focal loss, <strong>box_loss</strong> = bounding box regression loss. Validation metrics shown: <strong>mAP@50, mAP@50â€“95, Precision, Recall</strong>.
          <br /><span class="dual">
            í•™ìŠµ/ê²€ì¦ ê³¡ì„ (W&B ê¸°ë¡). <strong>cls_loss</strong>(ë¶„ë¥˜ ì†ì‹¤), <strong>dfl_loss</strong>(ë¶„í¬ ì´ˆì  ì†ì‹¤), <strong>box_loss</strong>(ë°”ìš´ë”© ë°•ìŠ¤ ì†ì‹¤). ê²€ì¦ ì§€í‘œ: <strong>mAP@50, mAP@50â€“95, Precision, Recall</strong>.
          </span>
        </figcaption>
      </figure>

      <figure>
        <img src="yolov12.jpg" alt="YOLOv12 Inference on Test Image" class="screenshot zoomable">
        <figcaption>
          Example of YOLOv12 inference on a test image, showing predicted bounding boxes, class labels, and confidence scores for medical pills.
          <br /><span class="dual">
              í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ YOLOv12 ì¶”ë¡  ì˜ˆì‹œ. ì˜ì•½í’ˆ ê°ì²´ì˜ ì˜ˆì¸¡ëœ ë°”ìš´ë”© ë°•ìŠ¤, í´ë˜ìŠ¤ ë¼ë²¨ ë° ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
          </span>
        </figcaption>
      </figure>
    </section>

    <!-- ===== PERCLASS ===== -->
    <section id="perclass">
      <h2>Sample Per-Class Metrics</h2>
      <p>Selected per-class metrics from external validation:</p>

      <table class="metric-table" aria-label="sample per-class metrics">
        <thead>
          <tr>
            <th>Class</th><th>Images</th><th>Instances</th><th>Precision</th><th>Recall</th><th>mAP@0.5</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>ë¬´ì½”ìŠ¤íƒ€ì •(ë ˆë°”ë¯¸í”¼ë“œ)</td><td>200</td><td>200</td><td>0.417</td><td>0.990</td><td>0.416</td></tr>
          <tr><td>ì„ë¡œì¼ˆì • 100mg</td><td>207</td><td>207</td><td>0.463</td><td>0.995</td><td>0.496</td></tr>
          <tr><td>ì•„ë¹Œë¦¬íŒŒì´ì • 10mg</td><td>224</td><td>224</td><td>0.487</td><td>0.996</td><td>0.502</td></tr>
          <tr><td>ìì´í”„ë ‰ì‚¬ì • 2.5mg</td><td>243</td><td>243</td><td>0.490</td><td>0.959</td><td>0.500</td></tr>
          <tr><td>ê°€ë°”í† íŒŒì • 100mg</td><td>828</td><td>828</td><td>0.456</td><td>0.993</td><td>0.476</td></tr>
        </tbody>
      </table>

      <p style="text-align:center; color:#9ca3af; font-size:0.92rem;">
        Full per-class results and extended tables are available in the project report upon request.
        <br /><span class="dual">ì „ì²´ í´ë˜ìŠ¤ë³„ ê²°ê³¼ëŠ” í”„ë¡œì íŠ¸ ë³´ê³ ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</span>
      </p>
    </section>

    <!-- ===== REFLECTION ===== -->
    <section id="reflection">
      <h2>Reflection</h2>
      <p>
        Full tuning + retraining pipeline took â‰ˆ19 hours. Under resource constraints, prioritizing dataset diversity (merging TS sets) and external validation improved robustness.
        <br /><span class="dual">ì „ì²´ íŠœë‹ ë° ì¬í•™ìŠµì€ ì•½ 19ì‹œê°„ ì†Œìš”ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° ë‹¤ì–‘ì„± í™•ë³´ì™€ ì™¸ë¶€ ê²€ì¦ì´ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.</span>
      </p>
    </section>

    <!-- ===== CONTACT ===== -->
    <section id="contact">
      <h2>Contact | ì—°ë½ì²˜</h2>
      <p>
        ğŸ’¼ <a href="https://github.com/hiiamaaron" target="_blank">GitHub Profile</a><br />
        ğŸ”— <a href="https://www.linkedin.com/in/raffaello-sanzio-da-urbino-703897345/" target="_blank">LinkedIn</a><br />
        ğŸ“§ <a href="mailto:projectares777@gmail.com">projectares777@gmail.com</a><br />
      </p>
    </section>
  </main>

  <div class="footer">
    Â© 2025 Aaron â€” Built with â¤ï¸ using GitHub Pages
  </div>

  <!-- ===== LIGHTBOX JS ===== -->
  <div id="lightbox" onclick="this.style.display='none'">
    <img id="lightbox-img" src="">
  </div>

  <script>
    const zoomableImages = document.querySelectorAll('.zoomable');
    const lightbox = document.getElementById('lightbox');
    const lightboxImg = document.getElementById('lightbox-img');

    zoomableImages.forEach(img => {
      img.addEventListener('click', () => {
        lightbox.style.display = 'flex';
        lightboxImg.src = img.src;
      });
    });
  </script>
</body>
</html>